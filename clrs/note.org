#+title: CLRS Algorithm Learning



* Lecture 1

** Analysis of Algorithm

** Insert Sort

** Asymptotic Analysis

** Merge Sort


* Lecture 2 

** Asymptotic Notation

*** Big O notation
The upper bound.  
   
*** Omega notation
The Lower bound

*** Theta notation
The tight bounds
   
*** little-o  and little-omega notation
Unlike Big O and Big-omega notation which are  less or equal than and bigger or equal than,

little-o and little-omega are just less than and bigger than.

** Recurrences

*** Substitution method
General method:
1. Guss the form of the solution
2. Verity by induction
3. Solve for constants

*** Iterating the recurrence

*** Recursion tree

*** Master Method

     

* Lecture 3 

  This lecture is mainly about *Divide and Conquer*.

  DC is one of the several powerful techniques for algorithm design.
  DC could be analyzed using recurrences and the master method
  DC strategy often leads to efficient algos.


  The design paradigm:
  - *Divide* the problem(instance) into subproblem
  - *Conquer* the subproblems by solving them recursively
  - *Combine* subproblem solutions



** Binary Search

   The Algorithm:
   - *Divide*: check the middle element
   - *Conquer*: recursively search 1 subarray
   - *Combine*: Trivial


** Powering a number

*** The Naive Algorithm

*** The Divide-and-Conquer Algorithm



** Fibonacci number
      

*** TODO Fibonacci is everywhere in nature
    Why is Fibonacci so important? Take a look at the wikipedia.


*** The naive recursive algorithm
    Use the standard formula to recursively get the final value.

    Complexity: Omega(Phi^n), Phi is the _golden ratio_ ( (1+ 5^(1/2))/2 ).


*** The bottom-up way

    Compute F_0, F_1, F_2,... F_n in order, so that the current F_i can reuse the former result.

    Complexity: Theta(n)

*** Naive directed way
    There is also a direct computation method 
    which is not reliable on currenty hardware due to the floating-point arithmetic precision problem.
    
    F_n = round( (Phi^n) / (5^(1/2)) ).

    round(): round the expression value to the nearest integer.


*** Recursive squaring algorithm

    We use the theoorem of matrix multiplication to get the final result of F_n

    Complexity: Theta(lg n)


** Matrix multiplication
   
   C=A*B

*** Standard Algorithm
    The standard algorithm is a 3-level loops that could give a result c_ij

    Complexity: Theta(n^3)

*** The standard Divide-and-Conquer algorithm
    - *Divide*: split the matrix into submatrices
    - *Conquey*: get the result of the submatrice in C
    - *Combine*: reassembe the submatrices into C

      T(n)=8*T(n/2) + Theta(n^2)

      Complexity: Theta(n^3)

    
*** Strassen's Divide-and-Conquer Algorithm

    The above algorithm have 8 recursive multiplications. 
    Strassen provided a way to only use 7 recursive  mults.

    This is still a DC algorithm.
    T(n) = 7T(n/2) + Theta(n^2)

** VLSI tree layout
   
   *Problem*: Embed a complete binary tree with n leaves in a grid using minimal area

    



* Lecture 4

  This lecture is mainly about *Quick Sort*.

  
** The Quick Sourt Algorithm
   QuickSort is also a DC.

   Algorithm Steps:
   - *Divide*: choose a pivot from the array, and use it to partition the array into 2 subarrays.
     such that, all elements of one  subarray is all  less than or equal to the /pivot/,
     and all elements in the other subarray is all greater than or equal to the /pivot/.

   - *Conquer*: Recursively sort the two subarrays.

   - *Combine*: Trivial.

   The implementation:
   - *Partition*: this is the divide phase.


** Analysis of quicksort
   
*** Worst-case of quicksort
    - One side of the partition always has no elements.



*** Best-case of quicksort
    - when each partition always have equal size.



*** Other cases of quicksort

    Even for the Almost-best case:
     T(n) = T(n/10) + T(9/10 * n) + Theta(n)

    and for the lucky-unluck case:
    L(n) = 2U(n/2) + Theta(n)
    U(n) = L(1) + L(n -1) + Theta(n)


    The complexity of the quicksort algorithm is also: O(nlgn)


** Randomized quicksort
   
   - Choose a random position element as the *pivot*.

   Since there are n conditions for the sort, so we use the expectation to show the time complexity.
   And the time complexity is also O(nlgn)

 
** Quicksort in practice
   - Quicksort is a great general-purpose sorting algorithm
   - Quicksort is typically over twice as fast as merge sort
   - Quicksort can benefit substantially from code tuning
   - Quicksort behaves well even with caching and virtual memory

   

* Lecture 5


** The lower bound of sorting
   
   The Defination of *comparison sort*: only use comparisons to determine the relative order of elements.

   The problem: Is O(nlogn) the best we can do? 
   *Decision Tree*  could help us answer this question.




** Decision Tree model
   A decision tree can model the execution of any comparision sort:
   - One Tree for each input size /n/
   - View the algorithm as splitting whenever it compares two elements
   - The tree contains the comparisons along all possible instruction traces
   - The running time of t he algorithm = the length of the path taken
   - Worst-case running time = height of tree
   
   *Theorem*: Any decision tree that can sort n elements must have height Omega(nlgn)



** Sorting in Linear time 


*** Counting sort
    
    - Init an array C with 0, the size of the array C is the range of the array element value range
    - Count each value apperance time, and record it in C
    - Replace each element in C with the accumulate value start from the first element.
    - Init a new array B with the same size of A.
    - From the end of A to the start, assign B[C[A[j]]] with A[j], and minus C[A[j]] by 1.

    The final array B is the result.


**** TODO The code of counting sort



*** Radix Sorting
    Radix sort is a digit-by-digit sort, sort on /least-significant digit first/ with auxiliary /stable/ sort.
    




* Lecture 6
  
  This lecture is mainly about *Order Statistics*.
  
  
** How to find the /i/th smallest number of n elements in the list?

   The /i/ can be:
   - =i = 1=: minimum
   - =i = n=: maximum
   - =i = (n+1)/2=: median. (assum that (n+1) is odd )
   - i: other value



*** Naive algorithm

    The naive algorithm:
    - Sort the list. By using mergesort or heapsort, the worst-case run time is /Theta(nlgn)/
    - find the element of index /i/
    


*** Randomized Divide-and-conquer Algorithm
    
    This algorithm derived from the randomized quicksort:
    - Reuse the Random-partition: which pick a pivot, and partition the array
    - Choose one partition to continue finding the final result.
      Unlike quicksort which will conquer each partition.


    For the expected time of ranomized select, we should also use the expection analysis method.     
    The average case, lucky-unlucky case, best case is all O(nlgn). 
    However, the worst case is O(n^2).


    The randomized order-statistic selection:
    - works fast: can be done lin linear expected time
    - It is an excellent algorithm in practice
    - The worst case is very bad, Theta(n^2)



*** Worst-case linear-time order statistics
    The main idea is to generate a good pivot recursively.

    The select(i,n) steps:
    - Divide  the n elements into groups of 5.
      Find the median of each 5-element group by rote.
    - Recursively SELECT the median x of the n/5 group medians to be the pivot
    - Partition around the pivot x. Let =k = rank(x)=
    - if =i = k= then return x
      else
        if =i < k=
          then recursively select the ith smallest element in the lower part
        else recursively select the (i-k)th smallest element in the upper part.


   After the median partition of 5-element group, and among all median of all groups. 
   It is sure that the pivot that have been selected can divide the list into a nearly middle partition,
   so that no worst-case will happen.

    


     



* Lecture 7
  
  This lecture is about *Hashing Basic*.


** The Symbol Table problem
   
   It is common for a program to maintain a table S that can:
   - =INSERT(S,x)=
   - =DELETE(S,x)=
   - =SEARCH(S,k)=

   The problem is:
   - How to maintain such a table
   - How to make the table efficiently support INSERT,DELETE, and SEARCH operations.


   *Direct-Access Table* Solution
   Allocate a table which uses the key directly as the index.
   The table should keep space for all the key-range index.
   If the key can be an arbitrary int value, the table should have 2^32 elements.
   It is too big.

** The Hashing Solutions
   *Hash function*: it is used to map the universe /U/ of akk keys into ={0,1,...,m-1}=.

   *Collision*: When a record to be inserted maps to an already occupied slot in the table,
   The collision occurs.



*** Hashing functions
    The assumption of simple uniform hashing is hard to guarantee.

    *Ideal Hashing function*
    - Should distribute the keys uniformly into the slots of the table
    - Regularity in the key distribution should not affect this uniformity.


**** Division Method
     
     =h(k) = k mod m=.

     

     *Deficiency*: Choose an prime number as m, and not too close to a power of 2 or 10 and not otherwise used prominently used,
     so that =h(k)='s result can associate with all the bits of /k/.
     
     *Annoyance*: Sometimes, making the table size a prime is iconvenient.

     counterexamples: if =m = 2^6=, =k = 1011000111011010= in binary.  =h(k)= only depends on the last 6 bits.
     other k with the same last 6 bits have the same hash result.


**** Multiplication method
     
     Assume the computer has w-bit words.
     =h(k) = (A*k mod 2^w) rsh (w -r)=

     - =A=: is an odd int in the range =2^(w-1) < A < 2^w=, do not pick it to the lower or upper bound.
     - =2^w=: Compare to div, it is very fast.
     - =rsh=: is the "bitwise right-shift" operator, it is fast
     


***** TODO What is modular wheel in multiplication method


*** Collision Resolution
    
**** Chaining
     
     Link records in the same slot into a list.

     *Worst Case*:
     Every key hashes to the same slot. 
     The Access time = Theta(n)

     *Average Case*:
     We need to make assumptions to analysis the average case.
     Here is the *simple uniform hashing*:
     - Each key /k/ is equally likely to be hashed to any slot of table /T/,
       independent of where other keys are hashed.


     Then, define the *load factor* of /T/ to be =Alpha = n/m = average number of keys per slot=.
     - /n/: the number of keys in the table
     - /m/: hte number of slots
       

     *Search cost*: the expected time for an unsuccessful search for a record with a given key is  =Theta(1+Alpha)=.
     =Alpha= is the cost of searching list.

     

**** Open Addressing
     
     It does not need storage outside of the hash table.
     - Insertion systematically probes the table until an empty slot is found
     - Hash function depends on both the key and probe number
     - The probe sequence should be a permutation of {0,1,...m-1}
     - The table may fill up, and deletion is difficult ( but not impossible)
     

       *Analysis of Open Addressing*
       For an open-addressed hash table with load factor =Alpha = n/m < 1=, 
       the expected number of probes in an unsuccessful search is at most =1/(1- Alpha)=.
       - If =Alpha= is constant, then accessing an open-addressed hash table taks constant time
       - If the table is half full, then the expected number of probes is =1/(1-0.5) = 2=.
       - If the table is =90%= full,  then the expected number of probes is =1/(1-0.9) = 10=.
       


***** Probing strategies for open addressing
      
      When collision occurs, we need a probing strategy to solve it.

      *Linear probing*:
      =h(k,i) = (H(k)+i) mod m= , simply increase the index.


      *Double Hashing*
      Define a new hash function to generate the new index when collision.


      
      
      

* Lecture 8

  This lecture is about *Advanced hashing technology* -- *Universal Hashing* and *Perfect Hashing*.
  

  *A weaknesss of hashing*:
  For any hash function /h/, there is a set of keys that can cause the average access time of a hash table to skyrocket.
  The keys make /h(k)/ point to the same slot /i/.
  *Solution*: Choose the hash function at random, independently of the keys.
  And this is *Universal hashing*.



** Universal Hashing
   
   *Definition*: /U/ is a universe of keys, /H/ is a finite collection of hash functions, each mapping /U/ to ={0,1,...m-1}= ,
   If the chance of a collision between /x/ and /y/ is /1\/m/ if we choose /h/ randomly from /H/.
   /H/ is *universal*.
   

   *Theorem*: choosing /h/ randomly from a universal set of hash function /H/.
   Suppose /h/ is used to hash /n/ arbitrary keys into the /m/ slots of a table /T/.
   Then for a given key /x/, we have: /E[#collisions with x] < n\/m/


   *How to construct a set of universal hash functions*:
   Let /m/ be prime. Decompose key /k/ into /r+1/ digits, each with value in the set /{0,1,..., m-1}/.
   That is =k = <k0,k1,...,kr>=, 0<=ki<m.  Randomly choose /r/ value from 0<= a < m.
   Define /ha(k) = sum (ai*ki mod m),  0 <= i <= r/.
   The set H={ha} is universal.



** Perfect Hashing
   
   Perfect means Search takes /Theta(1)/ time in the worst case.

   Two-level scheme with universal hashing at both levels. *There will be no collision at level 2!*

   Although we use an extra level of Hashing, since the expection of the 2nd level is /Theta(n)/, 
   the total storage of the 2-level hashing is also /Theta(n)/.

*** TODO how Universal hashing is implemented in code?
    



* Lecture 9

  This Lecture is about *Randomly built binary search trees*.



** Binary-Search-Tree Sort
   
   Binary search tree could be used for sort:
   - insert all the elements into a BST /T/
   - in-order tree walk of /T/

   The output of the above algorithm is the sorted list.

   
   BST sort have the same complextity as quicksort, 
   It is just a reorder of all the comparesions of quick sort.



   /This lecture is mainly about Math.... a little boring./



* Lecture 10
  This Lecture is about *Balanced Search Tree*.


** PROBLEM: Maintain a search-tree data  structure with always O(lg n) search time

    This type of tree could guarantee the height to be Theta(logn).
    - AVL Tree
    - 2-3 Tree
    - 2-3-4 Trees
    - B-Trees
    - Red-Black Tree

    And is mainly about *Red-Black tree*.

    
** Red-black tree
   This data structure requires an extra one-bit /color/ field in each node.

   It have the following 4 properties:
   - Every node is either red or black
   - The root and leaves (/NIL/'s) are black
   - If a node is red, then its parent is black.
   - All simple paths from any node /x/ to a descendant leaf have the same number of black nodes *= black-height(x)*


   *Height* ：
   A Red-black tree with /n/ keys has height  /h<=2lg(n+1)/.


   *All Queries ops run in /O(lgn)/ time on a red-black tree with /n/ nodes*:
   The queries /SEARCH/,/MIN/, /MAX/, /SUCCESSOR/, /PREDECESSOR/ all run in O(lgn) time on a red-black tree.


*** Query on Red-black tree
    Query on RB Tree is just like query on other BSTs.
    - SEARCH
    - MIN
    - MAX
    - SUCCESSOR
    - PREDECESSOR

    RB-Tree can always run the above ops in /O(lgn)/ time.
    
*** Modifying operation on RB-Tree
    
    For the INSERT and DELETE operations, 
    it may cause modification:
    - the operation itself
    - color changes
    - restructuring the links of the tree via *rotation*.
      


**** INSERTION
     *IDEA*: insert /x/ in tree. 
     - Color /x/ red firstly
     - Insert /x/ into the tree, just like other BSTs
     - Adjust the tree to satisfy the 4 properties.

     INSERTION may calls:
     - LEFT-ROTATE
     - RIGHT-ROTATE
     - recolor



* Lecture 11
  This Lecture is about *Augmenting Data Structures*.
  It is introduced by solving two problems:
  - Dynamic Order statistics
  - Interval trees
  

  This Lecture also give a 4-step methodology 
  on how to augment a data structure.
  - *Underlying data structure*: choosing an underlying data structure
  - *Additional information*: determining additional information to be maintained 
    in the underlying data structure
  - *Maintaining the information*: verifying that the additional information can be maintained
    fpr the basic modifying operations on the underlying
    data structure
  - *developing new operations*
  



** PROBLEM:Dynamic order statics problem

   Set /S/ is dynamic, The whole algorithm should support S-INSERTION, and S-DELETION.
   
   - =OS-SELECT(i,S)=: returns the /i/th smallest element in the dynamic set /S/.
   - =OS-RANK(x,S)=: returns the rank of x in the sorted order of /S/'s elements.

   *IDEA*: Use a red-black tree for the set /S/, but keep subtree sizes in the nodes.



** PROBLEM:Interval Tree
   
   Maintain a dynamic set of intervals. 
   For a given query interval /i/, find an interval in the set
   that overlaps /i/.


* Lecture 12
  
  This Lecture is about *Skip List*. Mainly about its:
  - Data Structure
  - Randomized Insertion
  - With-high-probability bound
  - Analysis of its complexity
  - Coin Flipping for its dynamic operation support
