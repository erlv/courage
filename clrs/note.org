#+title: CLRS Algorithm Learning


* Leture 1

** Analysis of Algorithm

** Insert Sort

** Asymptotic Analysis

** Merge Sort


* Leture 2 

** Asymptotic Notation

*** Big O notation
The upper bound.  
   
*** Omega notation
The Lower bound

*** Theta notation
The tight bounds
   
*** little-o  and little-omega notation
Unlike Big O and Big-omega notation which are  less or equal than and bigger or equal than,

little-o and little-omega are just less than and bigger than.

** Recurrences

*** Substitution method
General method:
1. Guss the form of the solution
2. Verity by induction
3. Solve for constants

*** Iterating the recurrence

*** Recursion tree

*** Master Method

     

* Lecture 3 

  This lecture is mainly about *Divide and Conquer*.

  DC is one of the several powerful techniques for algorithm design.
  DC could be analyzed using recurrences and the master method
  DC strategy often leads to efficient algos.


  The design paradigm:
  - *Divide* the problem(instance) into subproblem
  - *Conquer* the subproblems by solving them recursively
  - *Combine* subproblem solutions



** Binary Search

   The Algorithm:
   - *Divide*: check the middle element
   - *Conquer*: recursively search 1 subarray
   - *Combine*: Trivial


** Powering a number

*** The Naive Algorithm

*** The Divide-and-Conquer Algorithm



** Fibonacci number
      

*** TODO Fibonacci is everywhere in nature
    Why is Fibonacci so important? Take a look at the wikipedia.


*** The naive recursive algorithm
    Use the standard formula to recursively get the final value.

    Complexity: Omega(Phi^n), Phi is the _golden ratio_ ( (1+ 5^(1/2))/2 ).


*** The bottom-up way

    Compute F_0, F_1, F_2,... F_n in order, so that the current F_i can reuse the former result.

    Complexity: Theta(n)

*** Naive directed way
    There is also a direct computation method 
    which is not reliable on currenty hardware due to the floating-point arithmetic precision problem.
    
    F_n = round( (Phi^n) / (5^(1/2)) ).

    round(): round the expression value to the nearest integer.


*** Recursive squaring algorithm

    We use the theoorem of matrix multiplication to get the final result of F_n

    Complexity: Theta(lg n)


** Matrix multiplication
   
   C=A*B

*** Standard Algorithm
    The standard algorithm is a 3-level loops that could give a result c_ij

    Complexity: Theta(n^3)

*** The standard Divide-and-Conquer algorithm
    - *Divide*: split the matrix into submatrices
    - *Conquey*: get the result of the submatrice in C
    - *Combine*: reassembe the submatrices into C

      T(n)=8*T(n/2) + Theta(n^2)

      Complexity: Theta(n^3)

    
*** Strassen's Divide-and-Conquer Algorithm

    The above algorithm have 8 recursive multiplications. 
    Strassen provided a way to only use 7 recursive  mults.

    This is still a DC algorithm.
    T(n) = 7T(n/2) + Theta(n^2)

** VLSI tree layout
   
   *Problem*: Embed a complete binary tree with n leaves in a grid using minimal area

    



* Lecture 4

  This lecture is mainly about *Quick Sort*.

  
** The Quick Sourt Algorithm
   QuickSort is also a DC.

   Algorithm Steps:
   - *Divide*: choose a pivot from the array, and use it to partition the array into 2 subarrays.
     such that, all elements of one  subarray is all  less than or equal to the /pivot/,
     and all elements in the other subarray is all greater than or equal to the /pivot/.

   - *Conquer*: Recursively sort the two subarrays.

   - *Combine*: Trivial.

   The implementation:
   - *Partition*: this is the divide phase.


** Analysis of quicksort
   
*** Worst-case of quicksort
    - One side of the partition always has no elements.



*** Best-case of quicksort
    - when each partition always have equal size.



*** Other cases of quicksort

    Even for the Almost-best case:
     T(n) = T(n/10) + T(9/10 * n) + Theta(n)

    and for the lucky-unluck case:
    L(n) = 2U(n/2) + Theta(n)
    U(n) = L(1) + L(n -1) + Theta(n)


    The complexity of the quicksort algorithm is also: O(nlgn)


** Randomized quicksort
   
   - Choose a random position element as the *pivot*.

   Since there are n conditions for the sort, so we use the expectation to show the time complexity.
   And the time complexity is also O(nlgn)

 
** Quicksort in practice
   - Quicksort is a great general-purpose sorting algorithm
   - Quicksort is typically over twice as fast as merge sort
   - Quicksort can benefit substantially from code tuning
   - Quicksort behaves well even with caching and virtual memory

   

* Leture 5


** The lower bound of sorting
   
   The Defination of *comparison sort*: only use comparisons to determine the relative order of elements.

   The problem: Is O(nlogn) the best we can do? 
   *Decision Tree*  could help us answer this question.




** Decision Tree model
   A decision tree can model the execution of any comparision sort:
   - One Tree for each input size /n/
   - View the algorithm as splitting whenever it compares two elements
   - The tree contains the comparisons along all possible instruction traces
   - The running time of t he algorithm = the length of the path taken
   - Worst-case running time = height of tree
   
   *Theorem*: Any decision tree that can sort n elements must have height Omega(nlgn)



** Sorting in Linear time 


*** Counting sort
    
    - Init an array C with 0, the size of the array C is the range of the array element value range
    - Count each value apperance time, and record it in C
    - Replace each element in C with the accumulate value start from the first element.
    - Init a new array B with the same size of A.
    - From the end of A to the start, assign B[C[A[j]]] with A[j], and minus C[A[j]] by 1.

    The final array B is the result.


**** TODO The code of counting sort



*** Radix Sorting
    Radix sort is a digit-by-digit sort, sort on /least-significant digit first/ with auxiliary /stable/ sort.
    
